---
title: "NES8010 Quantitative Ecological Research Methods"
subtitle: "Multivariate analysis: constrained ordination and classification"
#author: "Roy Sanderson"
#date: "November 7, 2017"
output:
  word_document:
    reference_docx: template.docx
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=3.5, fig.height=3.5)
rm(list=ls())
library(tidyverse)
library(vegan)
```

## Introduction
In the last practical you used major __unconstrained__ ordination methods: linear (Principal Components Analysis PCA  via `rda` command) , unimodal (Correspondence Analysis CA via `cca` command) and non-metric (Non-metric multidimensional scaling NMDS via `metaMDS` command).  Typically your input table of data will be in the form of samples (sites) as rows and attributes (species) as columns, e.g.:

* samples x species matrix
* samples x gene descriptor
* samples x measures of chemical composition
* vegetation quadrats x plant species
* camera trap locations x different species in each camera trap

## Aims of Practical
You used the output from your ordinations in combination with environmental data to try and make a biological interpretation of the ordination graphs.  Constrained ordination methods use the environmental data as part of the ordination analysis itself, and are generally more powerful.  The main aims of this practical are to:

* demonstrate the problem of non-independence of sample scores in unconstrained ordination
* introduce constrained ordination and learn how to interpret the graphical display
* undertake formal statistical tests of environmental variables via permutation ANOVA
* introduce multivariate classification, and show how this can be combined with ordination outputs (optional exercise).

Most of today's practical will use in-built datasets within the `vegan` package, so that you can learn the analytical techniques quickly. However, we will also look at an example dataset collected by a project student at St Mary's Lighthouse, to give you an understanding of good practice when importing from Excel, and preparing data for `vegan` analysis.

## Before you begin
In RStudio, please create a new project in an empty folder, the create an empty R Script file in which you should include all your commands, comments etc.

## Problem of non-independence of sample scores
We will load an example dataset, from a survey of Scandinavian lichen pasture vegetation grazed by reindeer, plus associated soil environmental variables.  As this is an example data for the vegan package you can load it via the data command:

Load the vegan library to access some of the additional commands you will use in MVT, then use the example lichens dataset (see `?varespec` for more information).  After loading the lichen data `varespec` have a quick look at it in the RStudio viewer etc.

```{r load up lichen data}
library(vegan)
data(varespec)
```

You noticed a problem with an arch effect in this dataset in a previous practical, so we will use CA, correspondence analysis (via `cca` command), rather than PCA:

```{r ca of lichens}
varespec.ca <- cca(varespec)
plot(varespec.ca, display="sites")
plot(varespec.ca, display="species")
```
```{r summary for printing, eval=FALSE}
summary(varespec.ca)
```
The summary command produces a lot of output.  As usual focus on CA1 and CA2, and work out the % variation explained by these two axes.

In today's practical for speed we will just use the default plotting routines, but remember that much better plots are possible via `ggplot2` + `ggrepel` libraries, or by exporting to Excel. In the last practical you learnt how to extract samples (sites) scores and compare them with potential environmental or management explanatory variables after doing the ordination.  A problem with this method of 'post-ordination' analysis is that it assumes that the score for any sample (site) in Axis 1 is independent of all the other samples in Axis 1.  This is an assumption with all x vs y analyses; it is assumed that all the y-values are independent.  __In ordination they are not.__  For example, note the effect of removing sample number 28 (row number 9 in your dataset):

```{r effect of removing one sample}
varespec28 <- varespec[-9,]  # delete the ninth row (sample code 28)
varespec28.ca <- cca(varespec28)

# Now compare the plots with and without sample 28:
plot(varespec.ca, display="sites", main="Original")
plot(varespec28.ca, display="sites", main="Without sample code 28")
```

Notice how all the sample points move quite dramatically; both Axis 1 and Axis 2 have changed to mirror images of the original following the removal of just one sample.  The relative positions of the remaining samples have also changed slightly.  Clearly the sample scores are not independent of each other, as we assumed in the last practical when you first did an ordination, and then undertook a separate analysis (e.g. via a linear model) of the ordination scores and environmental data.


## Constrained ordination
Constrained ordination differs in that the explanatory variables are included in the ordination analysis itself.  These explanatory variables might be environmental, management, experimental, spatial or temporal (time-based).  Analysis of the effects of all these variables separate from the initial ordination can also be awkward and difficult to interpret, whereas including them in the ordination itself makes their effects simpler to understand.  We will continue with the `varechem` dataset, which contains environmental (chemical) data associated with the `varespec` species data.  As this is an example dataset within R, you can find additional information via `help(varechem)` or `?varechem`.

Constrained unimodal ordination, Constrained Correspondence Analysis (CCA) is also performed with the `cca` command, whilst constrained linear ordination, Redundancy Analysis (RDA) is done via the `rda` command.  You indicate to R that you are doing a constrained rather than unconstrained analysis by inclusion of the explanatory variables in the command.:

```{r varechem data}
data(varechem)   # Load up the environmental data
summary(varechem)   # Summary of varechem
```

You have fourteen potential explanatory variables; we will just use a few here for simplicity; you specify them to the right hand side of an R equation, similar to the `lm` and `glm` commands you have already encountered. The following `cca` line tells R to undertake a CCA, with *all* the species as "response" variables, and potassium, phosphorus, aluminium, soil pH and the amount of bare soil as "explanatory" variables. The result is stored in `varespec.cca`. *Question*: There are 44 species of plant in the varespec table. What is the problem (other than the time it would take!!) in running 44 separate linear models?

```{r cca of varespec with chemical}
varespec.cca <- cca(varespec ~ K + P + Al + pH + Baresoil, data=varechem)
plot(varespec.cca)
```

This produces a 'triplot' which displays the samples (sites), attributes (species) and environmental variables (arrows) in one plot.  If you find this too complex to interpret, you can display only two sets of information at a time:

```{r biplot sites bp, eval=FALSE}
plot(varespec.cca, display=c("sites", "bp"))
```

to display samples with environmental variables ("bp" is for "biplot" scores of environmental arrows).  The species equivalent is:

```{r biplot spp bp, eval=FALSE}
plot(varespec.cca, display=c("species", "bp"))
```

On your own: repeat the CCA but __partial-out__ or __condition__ the effect of Baresoil so that its effects are accounted for already in the final results.  Use the `Condition` option in CCA; i.e. `Condition(Baresoil)` in your CCA equation.  What is the impact of this? [__Hint:__ when you partial out a variable, the effects of the other variables is shown after taking account of the partialled-out variable]

## Interpretation of biplots or triplot
The closer together any two sample scores are to each other, the __more similar__ they are in their species (attribute) composition.  Two sample scores far apart from each other in the plot have few attributes in common and are relatively dissimilar from each other.  Likewise, two species that are plotted close together are found in the same samples etc.  The __importance__ of each environmental variable is represent by the __length__ of the arrows.  Hence P, Baresoil and Al are relatively important variables, whereas K and pH appear to be less important in these data.

The __relationship between__ environmental variables is indicated by the positions of arrows relative to each other.  For example:

* the Al and pH arrows are __parallel, with arrowheads in the same direction__, and therefore appear to be __strongly positively correlated__ with each other;
* Baresoil and pH are going in __with arrowheads in opposite directions__, and so are __strongly negatively correlated__;
* K and pH are at __90 degrees to each other__, and so are relatively __uncorrelated__ with each other in their effects.

Finally, you can link species (and samples) to the environment.  Species near an arrow head are positively associated with that environmental variable; species at the (undrawn) arrow tail in the opposite direction are negatively associated.  Species at 90-degrees are uncorrelated.  __Question__: look at your species and soil chemistry biplot.  Which species are positively correlated with the amount of bare soil, which species negatively, and which is uncorrelated?  (check with a demonstrator if unsure how to interpret the graph)

## What does each axis mean?
Now it is easier to interpret each axis, simply by looking at the plots.  For example, the Al arrow is almost parallel with CCA Axis 1 on the x-axis, so we can assume that Axis 1 increases with Aluminium, which is at low amounts with low Axis 1 values, and high amounts at high values of Axis 1.  Baresoil is at 45 degrees to both Axis 1 and Axis 2, so we can assume that there is some measure of this variable on both axes, with biggest amounts of bare soil at low values of Axis 1, and high values of Axis 2.

It is harder to interpret the 'importance' of each axis, in terms of the proportion of variation explained, as the analysis produces both constrained and unconstrained measures - you can display these with the __`summary(varespec.cca)`__ command, but as their interpretation is difficult, you may be better to focus on the plots themselves, and do a significance test on each axis.

## Significance tests on constrained ordinations
It is simple to undertake a significance test.  Formal Sums of Squares, Mean Squares etc. to calculate F-ratios are not possible, therefore a __permutation test__ is undertaken.  This shuffles (permutes) your species data repeatedly, repeats the analysis, and compares the output with the unshuffled original data.  If the environmental variables have no effect on the species, the real data will be little different from the random shuffled data.  If the unshuffled data is gives very different results from random shuffled data, then the explanatory variables are significant.  

To test the significance of each of your environmental variables, issue the following command `anova` command.  This may take a while to run depending on how powerful a PC you are using; the default is 999 permutations.  As it is a permutation test, __your results will differ from mine__, but should be approximately similar:  

```{r marginal anova of varespec.cca}
anova(varespec.cca, by="margin")
```

A marginal test is equivalent to a Type-II anova for a linear model; it takes into account the effects of the other variables, and the order in which you enter the variables into the analysis does not matter.

To test the significance of each constrained axis:

```{r axis anova of varespec.cca}
anova(varespec.cca, by="axis")
```

You can see that both Axis 1 and Axis 2 are highly significant.  You can increase or decrease the number of permutations by using the permutations= option.

If you have a large number of potential explanatory variables, vegan offers a stepwise selection method.  For example, there are actually 14 soil variables in the varechem dataset.  To put them all into your CCA you can undertake.  In the commands below, the   ~ .  indicates that all 14 soil chemical variables are to be included as explanatory variables:

```{r stepwise of cca, eval=FALSE}
varespec.bigcca <- cca(varespec ~ . , data=varechem)
varespec.mincca <- ordistep(varespec.bigcca)
```

This produces lots of screen output as it thins down the original large number of explanatory variables to the minimum set of most plausible ones.  It will generally retain some non-significant variables for you to consider more fully, due to the known problems of stepwise techniques.

## Constrained ordination with factors
The interpretation and display of a constrained ordination that includes factors is slightly different from that of continuous variables.  Here we will analyse some of the example `dune` and `dune.env` data that you used in the last practical, the latter containing a mixture of continuous variables, categorical factors, and ordered factors; again after loading them up use the `summary` command and `View` them in the RStudio data browser.

```{r load dune data}
data(dune)
data(dune.env)
```

The depth of the A1 soil horizon is a continuous variable, Management type is a categorical variables.  Moisture and Manure and Use are ordered factors.  Note that factors, even if coded numerically, do not produce Mean, Median etc. values in your `summary`.

For simplicity, we will simply include A1 soil horizon and Management type in the constrained analysis.  We will do a __linear__ constrained analysis, as you did a PCA (linear) on the dune data initially.  __Remember__: CA and CCA are _unimodal_ methods, whilst PCA and RDA are _linear_ methods. Again, we use the `rda` command, but now with explanatory variables.  By including explanatory variables, R automatically performs a RDA or redundancy analysis:

```{r rda of dune, eval=FALSE}
dune.rda <- rda(dune ~ A1 + Management, data=dune.env)
plot(dune.rda, display=c("sites", "cn"))
plot(dune.rda, display=c("species", "cn"))
```

This displays your sample scores and environmental variables, and a second graph of species scores and environmental variables.  As before, the A1 variable (soil depth), a continuous variable, is represented by an arrow, its length suggesting that it is an important explanatory variable in the analysis.  The Management variable, being a categorical factor, is now displayed by points.  Management techniques BF (biological farming) and HF (hobby farming) are similar to each other, and differ from NM (nature conservation management) along the RDA Axis 1 (x-axis).  SF (standard farming) unsurprisingly differs from all three of the others, along the second RDA Axis 1 (y-axis).

__*On your own*__: what is the statistical significance of A1 and farm Management?  Do permutation tests on both the terms in your constrained ordination, and the axes.

## Summary
If you have a table of data, Y, of samples (rows) by attributes (columns),  you can undertake unconstrained analyses using:

```{r examples rda, eval=FALSE}
rda(Y)     # Linear unconstrained: Principal Components Analysis
```

Sometimes the arch effect is a problem, in which case try:

```{r examples cca metaMDS, eval=FALSE}
cca(Y)     # Unimodal unconstrained: Correspondence Analysis
metaMDS(Y) # Non-metric distance-based: Non-metric MDS
```

If you also have a table of explanatory data, X, that contains variables x1, x2, x3, you can undertake constrained analyses using:

```{r constrained rda or cca, eval=FALSE}
rda(Y ~ x1 + x2 + x3, data=X)   # Linear: Redundancy Analysis
cca(Y ~ x1 + x2 + x3, data=X)   # Unimodal: Constrained CA 
```

To display:

```{r example plot types, eval=FALSE}
plot(Y.rda)  # Triplot Samples, Attributes, explanatory variables
plot(Y.rda, display=c("sites", "bp"))   #Biplot=Samples, Arrows 
plot(Y.rda, display=c("sites", "cn"))   #Biplot=Samples, Centroids 
plot(Y.rda, display=c("species", "cn")) #Biplot=Attribut,Centroids
```

To test significance:

```{r example anova, eval=FALSE}
anova(Y.rda, by="axis")    # Test each constrained axis
anova(Y.rda, by="margin")  # Test explanatory variables
```


## Useful ordination graphics
Part of the skill in ordination is to display them in ways that aid their biological interpretation.  Here you will learn how to put coloured polygons around groups of points in your ordination graph to help you interpret them.  Use the dune and dune.env dataset for these examples.

Summarise the dune data then do a PCA as before:

```{r example dune not eval, eval=FALSE}
summary(dune.env)  # Notice the three categories for Use
dune.pca <- rda(dune)
plot(dune.pca, display="sites", type="points")
```

The `type="points"` option ensures that only simple points and not text are produced in the graph for clarity.  Now produce an annotated plot based on the farm management (hayfield, hay-pasture, or pasture):

```{r example ordihull not eval, eval=FALSE}
with(dune.env, ordihull(dune.pca, Use, show.groups="Hayfield", col="blue"))
with(dune.env, ordihull(dune.pca, Use, show.groups="Haypastu", col="green"))
with(dune.env, ordihull(dune.pca, Use, show.groups="Pasture", col="red"))
```
Complete your diagram by adding a figure legend at the top-right of the plot; the next line looks complicated but it simply puts the legend at the top right of the plot, `lty` stands for line type (`lty=1` is a solid line), and you specify the colours (in the right order to match the ones used in ordihull above!).

```{r add legend interactively, eval=FALSE}
legend(locator(1), legend=c("Hayfield", "Hay pasture", "Pasture"),  col=c("blue", "green", "red"), lty=1)
```

and click on your graph in the position where you want to place your legend:
```{r produce graph but hide code, echo=FALSE}
data(dune)
data(dune.env)
dune.pca <- rda(dune)
plot(dune.pca, display="sites", type="points")
with(dune.env, ordihull(dune.pca, Use, show.groups="Hayfield", col="blue"))
with(dune.env, ordihull(dune.pca, Use, show.groups="Haypastu", col="green"))
with(dune.env, ordihull(dune.pca, Use, show.groups="Pasture", col="red"))
legend(x=1.5, y=-1, legend=c("Hayfield", "Hay pasture", "Pasture"),  col=c("blue", "green", "red"), lty=1, cex=0.5)
```

Remember that you can use the `ggvegan` package to improve the quality of your constrained ordination plots. You can also produce a similar plot with the `stat_chull` (for convex hull), in fewer lines. This is an extra function, so I have created a separate R script that you should download from Blackboard and `source` to activate the function first:

```{r with stat_chull}
source("H:/Downloads/stat_chull.R") # Assuming in your Downloads folder

# Create a dataframe with PC site scores and Use
dune.sco <- data.frame(scores(dune.pca, display="sites"), Use=as.factor(dune.env$Use))

ggplot(dune.sco, aes(x=PC1, y=PC2, fill=Use)) +
  geom_point() +
  stat_chull(alpha=0.5) + # alpha changes the transparency
  theme_classic()
```

If you successfully install the `ggvegan` package (optional) you may want to experiment with plotting constrained ordinations such as CCA or RDA results.

## Classification

### Aims
These exercises will allow you to explore more of the theory that is covered in the final lectures of the module, in particular classification of multivariate data (and its display), and ways of overlaying classification results onto ordinations to aid their interpretation.

## Multivariate classification or clustering
The vegan package has a number of functions that allow you to do hierarchical classifications of species x samples data tables, and to sort your original tables of data to show the main trends.  Packages such as MASS (default) and labdsv also provide functions for fuzzy clustering and indicator species analysis.  We will focus on the tools available in vegan, using its example "dune" dataset.

Many classification methods are based on measuring 'dissimilarity' between samples, based on the species composition.  Therefore clustering is often a two-step process, involving the construction of a dissimilarity table, and then using an appropriate method to split this table up into separate classes:

```{r dune dist for printing, eval=FALSE}
library(vegan)
data(dune)
dune.dis <- vegdist(dune)
dune.dis
```
```{r dune dist for calc, echo=FALSE}
library(vegan)
data(dune)
dune.dis <- vegdist(dune)
```

The table above is a 'triangular matrix' showing the dissimilarity in species composition.  Only half the matrix is needed as this top-right triangle would contain the same information.  Vales range between zero and one, with sites that are similar to each other having the lowest values (lowest dissimiliarity).

There are a number of ways of calculating ecological dissimilarity; the default used by vegdist is the Bray-Curtis index; other common ones you may see used are the Euclidean index and the Gower index (see the help page for the vegdist function for the full list of indices available).
Now that you have calculated the dissimilarity values you can classify and plot the results.  Hierarchical classification produces a dendrogram or tree-like graph to describe the classification; the shape of this graph differs depending on the algorithm used, but I usually find the 'average' linkage method (based on the average dissimilarity between groups of sites) the most robust:

```{r classify dune data}
dune.cla <- hclust(dune.dis, method="average")
plot(dune.cla)
```

Reading the dendrogram from top to bottom, the first division splits off samples 14, 15, 16 and 20 from all the others, so that they appear to be in a group on their own.  The next division separates samples 17 and 19 from the remainder.  Then site 1 is in a group of its own.  Finally there are two large groups, one containing sites 3, 4, 8, 9, 12 & 13; the second sites 2, 5, 6, 7, 10, 11 & 18.

Other common methods of clustering are single and complete linkage:

```{r single and complete cluster}
dune.cls <- hclust(dune.dis, method="single")
dune.clc <- hclust(dune.dis, method="complete")
```

Display the classification trees for these methods and notice how the shape of the trees changes.  Which is the best method?  One simple approach is to measure the correlation between your original dissimilarity matrix and the inter-group dissimilarity at each step up the classification tree via the cophenetic function:

```{r cophenetic correlations, eval=FALSE}
cor(dune.dis, cophenetic(dune.cla))
cor(dune.dis, cophenetic(dune.cls))
cor(dune.dis, cophenetic(dune.clc))
```
Based on the correlations above, which do you think is the best of the three classification methods on these data?

The exact number of classes you end up with depends both on the classification method used, and the height you decide to divide your dendrogram at.  If the height is set at 0.7 we would have 3 classes for example, but the decision on the number of classes is subjective and your judgement!  Using the complete-linked classification and 3 classes we can do the following:

```{r display dune.clc with 3 classes, eval=FALSE}
plot(dune.clc)
rect.hclust(dune.clc, 3)
dune.grp <- cutree(dune.clc, 3)
```
```{r display dune.clc with 3 classes not echo, echo=FALSE}
plot(dune.clc)
rect.hclust(dune.clc, 3)
dune.grp <- cutree(dune.clc, 3)
```

Look at the contents of `dune.grp` and you will see that the `cutree` command produces the classes for each of your samples based on how you have decided to cut your dendrogram into 3 groups.

You can use this classification in the same way as any other factor for your dune vegetation data.  For example, do show how the depth of the A1 soild horizon differs amongst your three classes (__Note:__ include `as.factor(dune.grp)` in the `ggplot` call to ensure that the numeric codes are treated as categorical):

```{r boxplot of dune groups}
data(dune.env)

ggplot(dune.env, aes(x=as.factor(dune.grp), y=A1)) +
   geom_boxplot() +
   xlab("Classification group") +
   ylab("Depth of A1 soil horizon") +
   theme_classic()
```

Likewise, apply your classification to a PCA:

```{r dune pca with ordihull for schedule, eval=FALSE}
dune.pca <- rda(dune)
plot(dune.pca, dis="sites")

ordihull(dune.pca, groups=dune.grp, show.groups=1, col="red")
ordihull(dune.pca, groups=dune.grp, show.groups=2, col="green")
ordihull(dune.pca, groups=dune.grp, show.groups=3, col="blue")

legend(locator(), legend=c("Class 1", "Class 2", "Class 3"), col=c("red", "green", "blue"), lty=1)
```
```{r dune pca with ordihull eval not echo, echo=FALSE}
dune.pca <- rda(dune)
plot(dune.pca, dis="sites")

ordihull(dune.pca, groups=dune.grp, show.groups=1, col="red")
ordihull(dune.pca, groups=dune.grp, show.groups=2, col="green")
ordihull(dune.pca, groups=dune.grp, show.groups=3, col="blue")

legend(x=1.5, y=-1, legend=c("Class 1", "Class 2", "Class 3"), col=c("red", "green", "blue"), lty=1, cex=0.5)
```

Well done - the above graph shows a multivariate ordination (via PCA) of your dune data, overlaid with a multivariate classification (via hclust) of the same data.  You also know that the soil depth differs between the three classes, with the highest values (and greatest variability) in Class 2.  If you prefer, a similar graph can be produced using `ggplot` with the `stat_chull` function described earlier; you can use `geom_text_repel` if you want to add the sample numbers.


## Species classification
Just as you can classify the sites, so you can classify species; you simply have to transpose (turn through 90 degrees) the original dune data.frame via the `t()` function when calling the `vegdist` command:

```{r veg classification, eval=FALSE}
dunespp.dis <- vegdist(t(dune))
dunespp.clc <- hclust(dunespp.dis, method="complete")
plot(dunespp.clc)
```

Compare the dendrogram with your species PCA plot of these data.  Notice how in the dendrogram that species such as Sagpro and Junbuf, which are close together in the PCA plot are grouped close to each other in the classification etc.

### Other useful functions
The vegan package contains several useful additional functions.  For example, you may also wish to investigate a simple procedure for producing species x samples tables sorted by the results of an ordination.  This can make patterns in your raw data table much clearer.  See the example using the __`vegemite`__ (not marmite!) function in the vegan tutorial. I suggest you try this with the standard `dune` dataset. First create an unconstrained ordination (e.g. PCA, DCA, CA or NMDS), then give the `vegemite` function the original `dune` vegetation, and the results of your ordination to print the sorted species x samples table. For a really jazzy output, use the `tabasco` command which produces the same table but in the form of a colourful "heatmap" (please don't blame me for the silly names of these two functions!)

## Working with Excel data
So far you have used in-built datasets, but in practice you will work with data from Excel spreadsheets etc. The way in which these may have been stored (e.g. 'long' versus 'wide' format) may require minor data "wrangling" before you can analyse it with `vegan`. For multivariate analysis `vegan` expects 'wide' format, with column names and row names.

Download the spreadsheet `rocky_shore_wide.xlsx` onto your PC and open it in Excel. It differs slightly from the `rocky_shore.xlsx` spreadsheet you used last week, although it contains largely the same data. You may want to bring the two spreadsheets up, side-by-side, on the screen to compare them. The main differences with `rocky_shore_wide.xlsx` are that:

* It is in "wide" format. Each row is a sample, each column an attribute. You may receive data in this format, and it is the style expected by `vegan`. However, it is harder to plot using `ggplot`. The format of `rocky_shore.xlsx` you used last week is "long" format. You can convert between the two formats (see below).
* It contains an extra column called `Sname`, short for "sample name". This is a unique code for each replicate (a, b or c). When undertaking multivariate analyses it is essential that each row has a unique code. 
* It contains an extra explanatory variable, DistCause, short for "Distance from Causeway". One aim of this study was to see if disturbance from people from the causeway was having any impact.

Read the `data` sheet of `rocky_shore_wide.xlsx` into an object called `rockyshore_wide`. You did this last week, so you may have to check your notes to remind you. Remember to load the `readxl` library first.

```{r read rocky_shore_wide}
library(readxl)
rocky_shore_wide <- read_xlsx("rocky_shore_wide.xlsx", sheet="data")
```

You can check the class of the `rock_shore_wide` object using the `class` function. It is a `tibble` (showing as `tbl` in the listing. A `tibble` does not contain conventional rownames, whereas a `data.frame` does. Unfortunately, `vegan` requires the slightly simpler "standard" `data.frame`, with rownames indicating the replicates. To convert, first convert to a standard data.frame, then indicate that you want the variable `Sname` to be used as the rownames:

```{r tibble to dataframe}
rocky_shore_df <- data.frame(rocky_shore_wide)
rownames(rocky_shore_df) <- rocky_shore_df[,"Sname"]
rocky_shore_df <- rocky_shore_df[, -1] # Delete the Sname column as now tagged as rownames
```

Now that you have your data in a "wide" data.frame, you can split it into the species and environment, to apply the usual `vegan` multivariate methods:

```{r split into spp and eviron, echo=FALSE}
colnames(rocky_shore_df) # Check which column numbers contain the spp and environment
rocky_spp <- rocky_shore_df[, 8:19]
rocky_env <- rocky_shore_df[, 2:7]
```

Now that you have created these two data.frames, have a go at using some of the multivariate methods. Is there an effect of distance from causeway on the community composition of the species for example?

## Conversion of wide to long format and _vice versa_
If you want to convert your species data into a "long" format, to make it easier to interrogate for summary statistics etc., this is relatively easy:

```{r wide to long}
# First move the rownames into a column, then convert into a tibble, and finally rename the
# new column 'rowname' into 'Sname'
rocky_spp_tbl <- rocky_spp  %>%
   rownames_to_column() %>%
   as_tibble() %>%
   rename(Sname = rowname)
rocky_spp_tbl

# Second, convert your tibble to long format   
rocky_spp_long <- rocky_spp_tbl %>%
   pivot_longer(-Sname, names_to = "Species", values_to = "Count") %>% 
   filter(Count > 0) # Remove zero entries (optional)
rocky_spp_long
```

The long format is often easier for plotting (and can be more compact if, as here, the zero entries are removed, although this is optional). To convert back to "wide" format, use the `pivot_wider` function:

```{r pivotting long to wide}
rocky_spp_long %>% pivot_wider(names_from = "Species", values_from = "Count")
```
You'll notice that the above sets the values for species not present in a sample to `NA`. Look at the help for `pivot_wider` to determine how to replace the `NA` values with zeros.
